batch_size: 16
epochs: 5
gradient_accumulation_steps: 1
lr: 4.0e-5
warm_up: 0.01
lora_r: 8
lora_alpha: 32
save_interval: 200
log_interval: 1
bmt_cpu_offload: False
bmt_pre_load: True
bmt_lr_decay_style: 'cosine'
save_optim: True
save_rng: True
enable_flash_attn_models: False
eps: 1.0e-8
lora: True

enable_sft_dataset_dir: '/data2/yzd/FlagAI/convo_v2/'
enable_sft_dataset_file: 'sft_train_english_alpaca.jsonl'
