
![Aquila_logo](./img/Aquila.PNG)



# æ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰

æ‚Ÿé“Â·å¤©é¹°ï¼ˆAquilaï¼‰ è¯­è¨€å¤§æ¨¡å‹æ˜¯é¦–ä¸ªå…·å¤‡ä¸­è‹±åŒè¯­çŸ¥è¯†ã€æ”¯æŒå•†ç”¨è®¸å¯åè®®ã€å›½å†…æ•°æ®åˆè§„éœ€æ±‚çš„å¼€æºè¯­è¨€å¤§æ¨¡å‹ã€‚
- ğŸŒŸ æ”¯æŒå¼€æºå•†ç”¨è®¸å¯ã€‚Aquilaç³»åˆ—æ¨¡å‹çš„æºä»£ç åŸºäº Apache 2.0 åè®®ï¼Œæ¨¡å‹æƒé‡åŸºäºã€Šæ™ºæºAquilaç³»åˆ—æ¨¡å‹è®¸å¯åè®®ã€‹ï¼Œä½¿ç”¨è€…åœ¨æ»¡è¶³è®¸å¯é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œå¯ç”¨äºå•†ä¸šç›®çš„ã€‚
- âœï¸ å…·å¤‡ä¸­è‹±æ–‡çŸ¥è¯†ã€‚Aquilaç³»åˆ—æ¨¡å‹åœ¨ä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™åŸºç¡€ä¸Šä» 0 å¼€å§‹è®­ç»ƒï¼Œä¸­æ–‡è¯­æ–™çº¦å  40%ï¼Œä¿è¯æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå°±å¼€å§‹ç§¯ç´¯åŸç”Ÿçš„ä¸­æ–‡ä¸–ç•ŒçŸ¥è¯†ï¼Œè€Œéç¿»è¯‘è€Œæ¥çš„çŸ¥è¯†ã€‚
- ğŸ‘®â€â™€ï¸ç¬¦åˆå›½å†…æ•°æ®åˆè§„éœ€æ±‚ã€‚Aquilaç³»åˆ—æ¨¡å‹çš„ä¸­æ–‡è¯­æ–™æ¥è‡ªæ™ºæºå¤šå¹´ç§¯ç´¯çš„ä¸­æ–‡æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¥è‡ª1ä¸‡å¤šä¸ªç«™æºçš„ä¸­æ–‡äº’è”ç½‘æ•°æ®ï¼ˆå…¶ä¸­99%ä»¥ä¸Šä¸ºå›½å†…ç«™æºï¼‰ï¼Œä»¥åŠè·å¾—å›½å†…æƒå¨æœºæ„æ”¯æŒçš„é«˜è´¨é‡ä¸­æ–‡æ–‡çŒ®æ•°æ®ã€ä¸­æ–‡ä¹¦ç±æ•°æ®ç­‰ã€‚æˆ‘ä»¬ä»åœ¨æŒç»­ç§¯ç´¯é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œå¹¶æºæºä¸æ–­åŠ å…¥AquilaåŸºç¡€æ¨¡å‹åç»­è®­ç»ƒä¸­ã€‚
- ğŸ¯æŒç»­è¿­ä»£ï¼ŒæŒç»­å¼€æºå¼€æ”¾ã€‚æˆ‘ä»¬å°†ä¸æ–­å®Œå–„è®­ç»ƒæ•°æ®ã€ä¼˜åŒ–è®­ç»ƒæ–¹æ³•ã€æå‡æ¨¡å‹æ€§èƒ½ï¼Œåœ¨æ›´ä¼˜ç§€çš„åŸºç¡€æ¨¡å‹åŸºåº§ä¸Šï¼ŒåŸ¹è‚²æç¹å¶èŒ‚çš„â€œæ¨¡å‹æ ‘â€ï¼ŒæŒç»­å¼€æºå¼€æ”¾æ›´æ–°çš„ç‰ˆæœ¬ã€‚


æ‚Ÿé“ Â· å¤©é¹° Aquila æ¨¡å‹çš„æ›´å¤šç»†èŠ‚å°†åœ¨å®˜æ–¹æŠ€æœ¯æŠ¥å‘Šä¸­å‘ˆç°ï¼Œé¢„è®¡ 2023 å¹´ 6 æœˆåº•å‘å¸ƒã€‚è¯·å…³æ³¨å®˜æ–¹æ¸ é“æ›´æ–°ã€‚åŒ…æ‹¬ [FlagAI GitHubä»“åº“](https://github.com/FlagAI-Open/FlagAI/)ï¼Œ[FlagAI çŸ¥ä¹è´¦å·](https://www.zhihu.com/people/95-22-20-18)ã€[FlagAI å®˜æ–¹æŠ€æœ¯äº¤æµç¾¤](https://github.com/FlagAI-Open/FlagAI/blob/master/wechat-qrcode.jpg)ã€æ™ºæºç ”ç©¶é™¢å¾®ä¿¡å…¬ä¼—å·ã€æ™ºæºç¤¾åŒºå¾®ä¿¡å…¬ä¼—å·ã€‚


|   æ¨¡å‹          |  æ¨¡å‹ç±»å‹    | ç®€ä»‹  |  æ–‡ä»¶è·¯å¾„   |   å•ç‹¬ä¸‹è½½æ¨¡å‹æƒé‡  |  çŠ¶æ€   |  è®­ç»ƒæ‰€ç”¨æ˜¾å¡   |                                   
| :---------------- | :------- | :-- |:-- |   :-- | :-- | :-- | 
| Aquila-7B         | åŸºç¡€æ¨¡å‹ï¼Œ70äº¿å‚æ•°  |   Aquila åŸºç¡€æ¨¡å‹åœ¨æŠ€æœ¯ä¸Šç»§æ‰¿äº† GPT-3ã€LLaMA ç­‰çš„æ¶æ„è®¾è®¡ä¼˜ç‚¹ï¼Œæ›¿æ¢äº†ä¸€æ‰¹æ›´é«˜æ•ˆçš„åº•å±‚ç®—å­å®ç°ã€é‡æ–°è®¾è®¡å®ç°äº†ä¸­è‹±åŒè¯­çš„ tokenizerï¼Œå‡çº§äº† BMTrain å¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†æ¯” Magtron+DeepSpeed ZeRO-2 å°†è¿‘ï¼˜å€çš„è®­ç»ƒæ•ˆç‡ã€‚   | [./examples/Aquila/Aquila-pretrain](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila/Aquila-pretrain)  | [model.baai.ac.cn/model-detail/100098](http://model.baai.ac.cn/model-detail/100098) | å·²å‘å¸ƒ | Nvidia-A100 |
| Aquila-33B          |å·²å‘å¸ƒ  |    åŒä¸Š    | Nvidia-A100  | 
| AquilaChat-7B          |å·²å‘å¸ƒ  |    âœ…   |   Nvidia-A100   | 
| AquilaChat-33B           |å·²å‘å¸ƒ |   âœ…    |  Tianshu-BI-V100   |
| AquilaCode-7B-NV          | **æ•¬è¯·æœŸå¾…**  |   âœ…   | Nvidia-A100  |
| AquilaCode-7B-TS           |**æ•¬è¯·æœŸå¾…**  |    âœ…    | Nvidia-A100  | 


<table>
  <tr>
    <td>æ¨¡å‹</td>
    <td>æ¨¡å‹ç±»å‹</td>
    <td>ç®€ä»‹</td>
    <td style="width: 100px;">æ–‡ä»¶è·¯å¾„</td>
    <td style="width: 100px;">å•ç‹¬ä¸‹è½½æ¨¡å‹æƒé‡</td>
    <td>çŠ¶æ€</td>
    <td>è®­ç»ƒæ‰€ç”¨æ˜¾å¡</td>
  </tr>
  <tr>
    <td>Aquila-7B</td>
    <td>åŸºç¡€æ¨¡å‹ï¼Œ70äº¿å‚æ•°</td>
    <td rowspan="2">Aquila åŸºç¡€æ¨¡å‹åœ¨æŠ€æœ¯ä¸Šç»§æ‰¿äº† GPT-3ã€LLaMA ç­‰çš„æ¶æ„è®¾è®¡ä¼˜ç‚¹ï¼Œæ›¿æ¢äº†ä¸€æ‰¹æ›´é«˜æ•ˆçš„åº•å±‚ç®—å­å®ç°ã€é‡æ–°è®¾è®¡å®ç°äº†ä¸­è‹±åŒè¯­çš„ tokenizerï¼Œå‡çº§äº† BMTrain å¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†æ¯” Magtron+DeepSpeed ZeRO-2 å°†è¿‘ï¼˜å€çš„è®­ç»ƒæ•ˆç‡ã€‚</td>
    <td>https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila/Aquila-pretrain</td>
    <td>http://model.baai.ac.cn/model-detail/100098</td>
    <td>å·²å‘å¸ƒ</td>
    <td>Nvidia-A100</td>
  </tr>
  <tr>
    <td>14</td>
    <td>15</td>
    <td>16</td>
    <td>17</td>
    <td>18</td>
    <td>19</td>
  </tr>
  <tr>
    <td>20</td>
    <td>21</td>
    <td rowspan="2">22</td>
    <td>24</td>
    <td>25</td>
    <td>26</td>
    <td>27</td>
  </tr>
  <tr>
    <td>28</td>
    <td>29</td>
    <td>30</td>
    <td>31</td>
    <td>32</td>
    <td>33</td>
  </tr>
  <tr>
    <td>34</td>
    <td>35</td>
    <td rowspan="2">36</td>
    <td>37</td>
    <td>38</td>
    <td>39</td>
    <td>39</td>
  </tr>
  <tr>
    <td>40</td>
    <td>41</td>
    <td rowspan="3">42</td>
    <td>43</td>
    <td>44</td>
    <td>46</td>
  </tr>


</table>





## ç®€ä»‹/Overview
Aquilaè¯­è¨€å¤§æ¨¡å‹åœ¨æŠ€æœ¯ä¸Šç»§æ‰¿äº†GPT-3ã€LLaMAç­‰çš„æ¶æ„è®¾è®¡ä¼˜ç‚¹ï¼Œæ›¿æ¢äº†ä¸€æ‰¹æ›´é«˜æ•ˆçš„åº•å±‚ç®—å­å®ç°ã€é‡æ–°è®¾è®¡å®ç°äº†ä¸­è‹±åŒè¯­çš„tokenizerï¼Œå‡çº§äº†BMTrainå¹¶è¡Œè®­ç»ƒæ–¹æ³•ï¼Œåœ¨Aquilaçš„è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†æ¯”Magtron+DeepSpeed zero-2å°†è¿‘ï¼˜å€çš„è®­ç»ƒæ•ˆç‡ã€‚Aquilaè¯­è¨€å¤§æ¨¡å‹æ˜¯åœ¨ä¸­è‹±æ–‡é«˜è´¨é‡è¯­æ–™åŸºç¡€ä¸Šä»ï¼å¼€å§‹è®­ç»ƒçš„ï¼Œé€šè¿‡æ•°æ®è´¨é‡çš„æ§åˆ¶ã€å¤šç§è®­ç»ƒçš„ä¼˜åŒ–æ–¹æ³•ï¼Œå®ç°åœ¨æ›´å°çš„æ•°æ®é›†ã€æ›´çŸ­çš„è®­ç»ƒæ—¶é—´ï¼Œè·å¾—æ¯”å…¶å®ƒå¼€æºæ¨¡å‹æ›´ä¼˜çš„æ€§èƒ½ã€‚ä¹Ÿæ˜¯é¦–ä¸ªæ”¯æŒä¸­è‹±åŒè¯­çŸ¥è¯†ã€æ”¯æŒå•†ç”¨è®¸å¯åè®®ã€ç¬¦åˆå›½å†…æ•°æ®åˆè§„éœ€è¦çš„å¤§è§„æ¨¡å¼€æºè¯­è¨€æ¨¡å‹ã€‚

The Aquila language model inherits the architectural design advantages of GPT-3 and LLaMA, replacing a batch of more efficient underlying operator implementations and redesigning the tokenizer for Chinese-English bilingual support. It upgrades the BMTrain parallel training method, achieving nearly 8 times the training efficiency of Magtron+DeepSpeed ZeRO-2 in the training process of Aquila. The Aquila language model is trained from scratch on high-quality Chinese and English corpora. Through data quality control and various training optimization methods, it achieves better performance than other open-source models with smaller datasets and shorter training times. It is also the first large-scale open-source language model that supports Chinese-English-Knowledge, commercial licensing, and complies with domestic data regulations.


<!-- æˆ‘ä»¬åŒæ—¶ä¹Ÿæ”¯æŒ[Huggingfaceå¹³å°](https://huggingface.co/BAAI)ã€‚

We also support [Huggingface](https://huggingface.co/BAAI). -->

æœ€ä½ç¡¬ä»¶éœ€æ±‚ï¼šè¿è¡ŒAquila-7Bç³»åˆ—éœ€è¦å†…å­˜30G, æ˜¾å­˜18Gï¼Œç”Ÿæˆæœ€å¤§é•¿åº¦ 2048 tokensã€‚

Minimum hardware requirements for running the Aquila-7b series, you need at least 30GB of memory and 18GB of GPU memory, and the maximum length of text generated should be 2048 tokens.

## æ¨¡å‹ç»†èŠ‚/Model details

|   æ¨¡å‹/Model          |  çŠ¶æ€/State    | èƒ½å¦å•†ç”¨/Commercial use?  |  æ‰€ç”¨æ˜¾å¡/GPU   |                                    
| :---------------- | :------- | :-- |:-- |   
| Aquila-7B         | å·²å‘å¸ƒ  |   âœ…   | Nvidia-A100  |  
| AquilaChat-7B          |å·²å‘å¸ƒ  |    âœ…    | Nvidia-A100  | 
| AquilaCode-7B-NV          |å·²å‘å¸ƒ  |    âœ…   |   Nvidia-A100   | 
| AquilaCode-7B-TS           |å·²å‘å¸ƒ |   âœ…    |  Tianshu-BI-V100   |
| Aquila-33B          | **æ•¬è¯·æœŸå¾…**  |   âœ…   | Nvidia-A100  |
| AquilaChat-33B           |**æ•¬è¯·æœŸå¾…**  |    âœ…    | Nvidia-A100  | 

æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç³»åˆ—æ›´é«˜æ•ˆçš„åº•å±‚ç®—å­æ¥è¾…åŠ©æ¨¡å‹è®­ç»ƒï¼Œå…¶ä¸­åŒ…æ‹¬å‚è€ƒ[flash-attention](https://github.com/HazyResearch/flash-attention)çš„æ–¹æ³•å¹¶æ›¿æ¢äº†ä¸€äº›ä¸­é—´è®¡ç®—ï¼ŒåŒæ—¶è¿˜ä½¿ç”¨äº†RMSNormã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å‡çº§äº†[BMtrain](https://github.com/OpenBMB/BMTrain)æŠ€æœ¯è¿›è¡Œè½»é‡åŒ–çš„å¹¶è¡Œè®­ç»ƒï¼Œè¯¥æŠ€æœ¯é‡‡ç”¨äº†æ•°æ®å¹¶è¡Œã€ZeROï¼ˆé›¶å†—ä½™ä¼˜åŒ–å™¨ï¼‰ã€ä¼˜åŒ–å™¨å¸è½½ã€æ£€æŸ¥ç‚¹å’Œæ“ä½œèåˆã€é€šä¿¡-è®¡ç®—é‡å ç­‰æ–¹æ³•æ¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚

Aquilaæ¨¡å‹æ‰€é‡‡ç”¨çš„tokenizeræ˜¯ç”±æˆ‘ä»¬ä»å¤´å¼€å§‹è®­ç»ƒçš„ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ã€‚æˆ‘ä»¬åœ¨å¤„ç†è‹±æ–‡ã€ä¸­æ–‡ä»¥åŠä»£ç æ•°æ®æ—¶ï¼Œé‡‡ç”¨äº†ä¸åŒçš„åˆ†è¯å™¨å¯¹ä¸€ä¸‡ä¸ªæ ·æœ¬è¿›è¡Œäº†æŠ½å–ã€‚éšåï¼Œæˆ‘ä»¬ç»Ÿè®¡äº†æ¯ä¸ªæ ·æœ¬çš„tokenæ•°é‡ï¼Œå¹¶å°†å…¶è®°å½•åœ¨è¡¨æ ¼ä¸­ã€‚Aquila tokenizerä¸å…¶ä»–tokenizerçš„å‚æ•°å¯¹æ¯”è§ä¸‹è¡¨:

We used a series of more efficient low-level operators to assist with model training, including methods referenced from [flash-attention](https://github.com/HazyResearch/flash-attention) and replacing some intermediate calculations, as well as using RMSNorm. Building upon this foundation, we applied the [BMtrain](https://github.com/OpenBMB/BMTrain) for lightweight parallel training, which utilizes methods such as data parallelism, ZeRO (zero redundancy optimizer), optimizer offloading, checkpoint and operation fusion, and communication-computation overlap to optimize the model training process.

The tokenizer used in the Aquila model was trained from scratch by us and supports both English and Chinese. We used different tokenizers to extract ten thousand data samples from English, Chinese, and code data respectively, obtained the count of tokens for each sample, and also included it in the table. The parameters of this tokenizer are compared to those of other tokenizers in the table below:



| æ¨¡å‹/Model | è¯è¡¨å¤§å°/Vocab size | è¯´æ˜/Note |è‹±æ–‡å¹³å‡tokensé‡/Avg tokens(English)| ä¸­æ–‡å¹³å‡tokensé‡/Avg tokens(Chinesse)|ä»£ç å¹³å‡tokensé‡/Avg tokens(code)  |
|  -----  | ----  | -----  | ----  | -----  | ----  | 
| GPT2 | 50527 | bpe|1717 | 1764|2323 |
| LlaMA | 32000 | sp(bpe)|1805| 1257|1970 |
| Aquila | 100000 | bpe|1575 | 477|1679 |


## è®­ç»ƒæ•°æ®é›†/Training data 
Aquilaé¢„è®­ç»ƒä½¿ç”¨äº†Pileï¼Œ[RedPajama-Data-1T](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T), [Wikipedia](https://huggingface.co/datasets/wikipedia), [C4](https://huggingface.co/datasets/c4), æ‚Ÿé“ä¸­æ–‡æ•°æ®é›†ã€ç”µå­ä¹¦ã€ä¸“åˆ©ã€ç™¾ç§‘ã€è®ºå›, githubæ•°æ®ç­‰, è¯¦æƒ…å¯è§ä¸‹å›¾ã€‚

The Aquila-7B model was pretrained on Pileï¼Œ[RedPajama-Data-1T](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T), [Wikipedia](https://huggingface.co/datasets/wikipedia), [C4](https://huggingface.co/datasets/c4), Wudao Corpusã€e-bookã€Patent, encyclopedia, forum, github etc. Details are given in the figure below.

<!-- ![Screenshot](./img/data_dist.png) -->
Aquila ç³»åˆ—æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®ä¸å¼€æºï¼Œä½†æ•°æ®åˆ†å¸ƒæƒ…å†µå°†åœ¨å®˜æ–¹æŠ€æœ¯æŠ¥å‘Šä¸­å±•ç°ï¼ˆé¢„è®¡6æœˆåº•å‘å¸ƒï¼Œæ•¬è¯·æœŸå¾…ï¼‰ã€‚

The pre-training data of the Aquila series models are not open-sourced, but the data distribution will be presented in the official technical report (expected to be released by the end of June, stay tuned).


## ä½¿ç”¨æ–¹å¼/How to use

### 1. é¢„è®­ç»ƒ/Pre-training
#### Step 1: ä¿®æ”¹å‚æ•°/Modify Parameters

* `cd /examples/aquila`
* é…ç½®`hostfile`æ–‡ä»¶, å‚è€ƒ[è¿™é‡Œ](../../doc_zh/TUTORIAL_8_ENVIRONMENT_SETUP.md#aé…ç½®hostfilehostfile-ä¸­çš„v100-1-ä¸sshconfig-å¯¹åº”) ; Configure the `hostfile` file, refer to [here](../../docs/TUTORIAL_8_ENVIRONMENT_SETUP.md)
* é…ç½®`bmtrain_mgpu.sh`æ–‡ä»¶, å°†`SCRIPT_FILE`æ”¹æˆ`aquila_pretrain.py`; configure the `bmtrain_mgpu.sh` file, change `SCRIPT_FILE` to `aquila_pretrain.py`
* (å¯é€‰) åœ¨`Aquila-pretrain.yaml`æ–‡ä»¶é‡Œæ›´æ”¹å‚æ•° ; (optional) change parameters in `Aquila-pretrain.yaml`

| å‚æ•°å Parameter             | ç±»å‹ Type | æè¿° Description                                        |
|--------------------------------|------------|-------------------------------------------------------|
| batch_size | int   | æ¯æ¬¡è¿­ä»£è®­ç»ƒæ—¶ï¼Œä»æ•°æ®é›†ä¸­æŠ½å–çš„æ ·æœ¬æ•°ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå®ƒè¶Šå¤§ï¼Œå¤„ç†é€Ÿåº¦è¶Šå¿«ï¼Œä½†ä¼šå ç”¨æ›´å¤šçš„å†…å­˜; The number of samples extracted from the dataset for each iteration during training. Generally, a larger batch size can speed up processing but may also consume more memory                    |
| gradient_accumulation_steps | int   | åœ¨æ›´æ–°æ¨¡å‹æƒé‡ä¹‹å‰ï¼Œè¦å¯¹å¤šä¸ªå°æ‰¹æ¬¡è¿›è¡Œæ¢¯åº¦è®¡ç®—çš„æ¬¡æ•°ã€‚ä¸»è¦åº”ç”¨äºGPUæ˜¾å­˜è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨å°çš„batch_sizeï¼Œé€šè¿‡æ¢¯åº¦ç´¯ç§¯è¾¾åˆ°ä¸å¤§batch_sizeç›¸åŒçš„æ•ˆæœ; The number of samples extracted from the dataset for each iteration during training. Generally, a larger batch size can speed up processing but may also consume more memoryimages                  |
| lr | float   | æŒ‡æ§åˆ¶æ¨¡å‹æ›´æ–°å‚æ•°æ—¶çš„æ­¥é•¿æˆ–é€Ÿç‡ã€‚å­¦ä¹ ç‡è¿‡é«˜å¯èƒ½å¯¼è‡´æ¨¡å‹ä¸æ”¶æ•›ï¼Œè€Œå­¦ä¹ ç‡è¿‡ä½åˆ™å¯èƒ½å¯¼è‡´è®­ç»ƒæ—¶é—´è¿‡é•¿æˆ–è€…é™·å…¥å±€éƒ¨æœ€ä¼˜è§£; The step size or rate at which the model updates its parameters during training. A high learning rate may cause the model not to converge, while a low learning rate may result in long training times or being stuck in a local optimum                  |
| warm_up | float   | åˆå§‹å­¦ä¹ ç‡ä¸åŸå§‹å­¦ä¹ ç‡çš„æ¯”ä¾‹; The ratio between the initial learning rate and the original learning rate
| save_interval | int  | æ¨¡å‹ä¿å­˜çš„é—´éš”ï¼Œå³æ¯è®­ç»ƒå¤šå°‘ä¸ªiterationä¿å­˜ä¸€æ¬¡æ¨¡å‹ã€‚å½“è®­ç»ƒæ—¶é—´è¾ƒé•¿æ—¶ï¼Œä¿å­˜é—´éš”å¯ä»¥é¿å…å› çªç„¶ä¸­æ–­æˆ–å‡ºç°é”™è¯¯å¯¼è‡´è®­ç»ƒæˆæœå…¨éƒ¨ä¸¢å¤±; The interval at which the model is saved, i.e., how many iterations the model is saved during training. When training takes a long time, saving intervals can prevent all training achievements from being lost due to sudden interruptions or errors.                    |

* æˆ‘ä»¬çš„æ¼”ç¤ºæ•°æ®é›†æ”¾åœ¨`../indexed_dataset/data/demo_text_document`é‡Œã€‚ å¦‚æœæƒ³ä¿®æ”¹é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¯æ›´æ”¹`aquila_pretrain.py`é‡Œçš„`data_prefix`å‚æ•°; Our demo dataset is located in `../indexed_dataset/data/demo_text_document`. If you want to modify the pre-training dataset, you can change the data_prefix parameter in `aquila_pretrain.py`.
#### Step 2: å¯åŠ¨è®­ç»ƒ/Start training
å¯¹äºAquila-7Bæ¨¡å‹
```
bash dist_trigger_docker.sh hostfile Aquila-pretrain.yaml aquila-7b [å®éªŒå]
```   

å¯¹äºAquila-33Bæ¨¡å‹
```
bash dist_trigger_docker.sh hostfile Aquila-pretrain-33B.yaml aquila-33b [å®éªŒå]
```   

 æ¥ä¸‹æ¥ä¼šè¾“å‡ºä¸‹åˆ—ä¿¡æ¯ï¼Œæ³¨æ„`NODES_NUM`åº”è¯¥ä¸èŠ‚ç‚¹æ•°ç›¸ç­‰ï¼Œ`LOGFILE`æ˜¯æ¨¡å‹è¿è¡Œçš„æ—¥å¿—æ–‡ä»¶ï¼›The following information will be output. Note that `NODES_NUM` should be equal to the number of nodes, and `LOGFILE` is the log file for the model run.

![Screenshot](./img/info.jpg)

æˆåŠŸè®­ç»ƒä¹‹å‰èƒ½çœ‹åˆ°å¦‚ä¸‹ä¿¡æ¯(å…·ä½“å‚æ•°å¯èƒ½ä¸åŒ)ï¼› Before successful training, you may see the following information with parameters that may differ:

![Screenshot](./img/info2.jpg)
  
### 2. å¯ç›‘ç£å¾®è°ƒ/Supervised Fine-tuning(SFT)
#### Step 1: ä¿®æ”¹å‚æ•°/Modify Parameters
* `cd /examples/aquila`
* é…ç½®`hostfile`æ–‡ä»¶, å‚è€ƒ[è¿™é‡Œ](../../doc_zh/TUTORIAL_8_ENVIRONMENT_SETUP.md#aé…ç½®hostfilehostfile-ä¸­çš„v100-1-ä¸sshconfig-å¯¹åº”) ; Configure the `hostfile` file, refer to [here](../../docs/TUTORIAL_8_ENVIRONMENT_SETUP.md)
* é…ç½®`bmtrain_mgpu.sh`æ–‡ä»¶, å°†`SCRIPT_FILE`æ”¹æˆ`aquila_pretrain.py`; configure the `bmtrain_mgpu.sh` file, change `SCRIPT_FILE` to `aquila_pretrain.py`
* (å¯é€‰) åœ¨`Aquila-pretrain.yaml`æ–‡ä»¶é‡Œæ›´æ”¹å‚æ•° ; (optional) change parameters in `Aquila-pretrain.yaml`



#### Step 2: å¯åŠ¨å¯ç›‘ç£å¾®è°ƒ/Start SFT
```
cd ../Aquila-chat/
```
å¯¹äºAquila-7Bæ¨¡å‹ï¼š
```
bash dist_trigger_docker.sh hostfile Aquila-chat.yaml aquila-7b [å®éªŒå experiment name]
```
å¯¹äºAquila-33Bæ¨¡å‹:
```
bash dist_trigger_docker.sh hostfile Aquila-chat.yaml aquila-33b [å®éªŒå experiment name]
```
æ¥ä¸‹æ¥ä¼šè¾“å‡ºä¸‹åˆ—ä¿¡æ¯ï¼Œæ³¨æ„`NODES_NUM`åº”è¯¥ä¸èŠ‚ç‚¹æ•°ç›¸ç­‰ï¼Œ`LOGFILE`æ˜¯æ¨¡å‹è¿è¡Œçš„æ—¥å¿—æ–‡ä»¶ï¼›The following information will be output. Note that `NODES_NUM` should be equal to the number of nodes, and `LOGFILE` is the log file for the model run.

![Screenshot](./img/info.jpg)

æˆåŠŸè®­ç»ƒä¹‹å‰èƒ½åœ¨æ—¥å¿—é‡Œçœ‹åˆ°å¦‚ä¸‹ä¿¡æ¯(å…·ä½“å‚æ•°å¯èƒ½ä¸åŒ)ï¼› Before successful training, you may see the following information in the log file with parameters that may differ:

![Screenshot](./img/info2.jpg)

### 3. æ¨ç†/Inference

```python
import os
import torch
from flagai.auto_model.auto_loader import AutoLoader
from flagai.model.predictor.predictor import Predictor
from flagai.data.tokenizer import Tokenizer
import bminf

state_dict = "./checkpoints_in/"
model_name = 'aquila-7b' # 'aquila-33b'

loader = AutoLoader(
    "lm",
    model_dir=state_dict,
    model_name=model_name,
    use_cache=True)
model = loader.get_model()
tokenizer = loader.get_tokenizer()

model.eval()
model.half()
model.cuda()

predictor = Predictor(model, tokenizer)

text = "åŒ—äº¬åœ¨å“ªå„¿?"
text = f'{text}' 
print(f"text is {text}")
with torch.no_grad():
    out = predictor.predict_generate_randomsample(text, out_max_length=200, temperature=0)
    print(f"pred is {out}")

```




## è¯ä¹¦/License

Aquila-7Bå’ŒAquila-33Bå¼€æºæ¨¡å‹ä½¿ç”¨ [æ™ºæºAquilaç³»åˆ—æ¨¡å‹è®¸å¯åè®®](../../BAAI_Aquila_Model_License.pdf), åŸå§‹ä»£ç åŸºäº[Apache Licence 2.0](https://www.apache.org/licenses/LICENSE-2.0)ã€‚


Aquila-7B and Aquila-33B open-source model is licensed under [ BAAI Aquila Model Licence Agreement](../../BAAI_Aquila_Model_License.pdf). The source code is under [Apache Licence 2.0](https://www.apache.org/licenses/LICENSE-2.0)
