import os
import sys
import random
import itertools
from pathlib import Path

import torch
from torch.utils.data import Dataset
from torch.cuda.amp import autocast, GradScaler  # æ··åˆç²¾åº¦è®­ç»ƒ
from PIL import Image
from torchvision import transforms

from flagai.auto_model.auto_loader import AutoLoader

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =============== è®­ç»ƒå‚æ•°é…ç½® ===============
instance_data_dir = "./instance_images"  # å®ä¾‹å›¾ç‰‡ç›®å½•
instance_prompt = "smileğŸ˜"          # å®ä¾‹æç¤ºè¯ï¼ˆåŒ…å«ç‰¹æ®Šæ ‡è¯†ç¬¦ï¼‰

with_prior_preservation = False      # æ˜¯å¦ä½¿ç”¨å…ˆéªŒä¿ç•™
class_data_dir = "Mix"               # ç±»åˆ«å›¾ç‰‡ç›®å½•
class_prompt = "smile"                 # ç±»åˆ«æç¤ºè¯
prior_loss_weight = 1.0              # å…ˆéªŒæŸå¤±æƒé‡
num_class_images = 4                 # ç±»åˆ«å›¾ç‰‡æ•°é‡
resolution = 128                     # å›¾ç‰‡åˆ†è¾¨ç‡
center_crop = False                  # æ˜¯å¦ä¸­å¿ƒè£å‰ª

train_text_encoder = False           # æ˜¯å¦è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨
train_only_unet = True               # æ˜¯å¦ä»…è®­ç»ƒUNet

# è®­ç»ƒè¶…å‚æ•°
num_train_epochs = 10                # è®­ç»ƒè½®æ•°
batch_size = 2                       # æ‰¹æ¬¡å¤§å°
learning_rate = 5e-6                 # å­¦ä¹ ç‡
adam_beta1 = 0.9                     # Adamä¼˜åŒ–å™¨å‚æ•°
adam_beta2 = 0.999                   # Adamä¼˜åŒ–å™¨å‚æ•°
adam_weight_decay = 1e-2             # æƒé‡è¡°å‡
adam_epsilon = 1e-08                 # æ•°å€¼ç¨³å®šæ€§å¸¸æ•°

# =============== æ¨¡å‹åˆå§‹åŒ– ===============
# åŠ è½½AltDiffusion-m18æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹
auto_loader = AutoLoader(task_name="text2img",
                         model_name="AltDiffusion-m18")

model = auto_loader.get_model()      # è·å–æ¨¡å‹
tokenizer = model.tokenizer          # è·å–åˆ†è¯å™¨

# =============== æ•°æ®é›†å®šä¹‰ ===============
class DreamBoothDataset(Dataset):
    def __init__(
            self,
            instance_data_root,    # å®ä¾‹å›¾ç‰‡è·¯å¾„
            instance_prompt,       # å®ä¾‹æç¤ºè¯
            tokenizer,             # åˆ†è¯å™¨
            class_data_root=None,  # ç±»åˆ«å›¾ç‰‡è·¯å¾„
            class_prompt=None,     # ç±»åˆ«æç¤ºè¯
            size=512,              # å›¾ç‰‡å¤§å°
            center_crop=False,     # æ˜¯å¦ä¸­å¿ƒè£å‰ª
    ):
        self.size = size
        self.center_crop = center_crop
        self.tokenizer = tokenizer

        # å¤„ç†å®ä¾‹å›¾ç‰‡
        self.instance_data_root = Path(instance_data_root)
        if not self.instance_data_root.exists():
            raise ValueError("Instance images root doesn't exists.")
        self.instance_images_path = list(Path(instance_data_root).iterdir())
        self.num_instance_images = len(self.instance_images_path)
        self.instance_prompt = instance_prompt
        self._length = self.num_instance_images

        # å¤„ç†ç±»åˆ«å›¾ç‰‡ï¼ˆç”¨äºå…ˆéªŒä¿ç•™ï¼‰
        if class_data_root is not None:
            self.class_data_root = Path(class_data_root)
            self.class_data_root.mkdir(parents=True, exist_ok=True)
            self.class_images_path = list(self.class_data_root.iterdir())
            random.shuffle(self.class_images_path)
            self.num_class_images = len(self.class_images_path)
            self._length = max(self.num_class_images, self.num_instance_images)
            self.class_prompt = class_prompt
        else:
            self.class_data_root = None

        # å›¾ç‰‡é¢„å¤„ç†æµç¨‹
        self.image_transforms = transforms.Compose(
            [
                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),
                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),
                transforms.ToTensor(),
                transforms.Normalize([0.5], [0.5]),  # å½’ä¸€åŒ–åˆ°[-1, 1]
            ]
        )

    def __len__(self):
        return self._length

    def __getitem__(self, index):
        example = {}
        # åŠ è½½å®ä¾‹å›¾ç‰‡
        path = self.instance_images_path[index % self.num_instance_images]
        instance_image = Image.open(path)
        if not instance_image.mode == "RGB":
            instance_image = instance_image.convert("RGB")

        # åº”ç”¨é¢„å¤„ç†å¹¶è·å–æç¤ºè¯token
        example["instance_images"] = self.image_transforms(instance_image)
        example["instance_prompt_ids"] = self.tokenizer(
            self.instance_prompt,
            padding="do_not_pad",
            truncation=True,
            max_length=self.tokenizer.model_max_length,
        ).input_ids
        example["caption"] = self.instance_prompt

        # åŠ è½½ç±»åˆ«å›¾ç‰‡ï¼ˆå¦‚æœä½¿ç”¨å…ˆéªŒä¿ç•™ï¼‰
        if self.class_data_root:
            class_image = Image.open(self.class_images_path[index % self.num_class_images])
            if not class_image.mode == "RGB":
                class_image = class_image.convert("RGB")
            example["class_images"] = self.image_transforms(class_image)
            example["class_prompt_ids"] = self.tokenizer(
                self.class_prompt,
                padding="do_not_pad",
                truncation=True,
                max_length=self.tokenizer.model_max_length,
            ).input_ids

        return example

# =============== åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ ===============
train_dataset = DreamBoothDataset(
    instance_data_root=instance_data_dir,
    instance_prompt=instance_prompt,
    class_data_root=class_data_dir if with_prior_preservation else None,
    class_prompt=class_prompt,
    tokenizer=tokenizer,
    size=resolution,
    center_crop=center_crop,
)

def collate_fn(examples):
    # æ‰¹é‡å¤„ç†æ•°æ®
    input_ids = [example["instance_prompt_ids"] for example in examples]
    pixel_values = [example["instance_images"] for example in examples]
    captions = [example["caption"] for example in examples]

    # åˆå¹¶å®ä¾‹å’Œç±»åˆ«æ•°æ®ï¼ˆå¦‚æœä½¿ç”¨å…ˆéªŒä¿ç•™ï¼‰
    if with_prior_preservation:
        input_ids += [example["class_prompt_ids"] for example in examples]
        pixel_values += [example["class_images"] for example in examples]

    pixel_values = torch.stack(pixel_values)
    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()

    input_ids = tokenizer.pad({"input_ids": input_ids}, padding=True, return_tensors="pt").input_ids

    batch = {
        "input_ids": input_ids,
        "pixel_values": pixel_values,
        "caption": captions,
        "txt": captions
    }
    return batch

train_dataloader = torch.utils.data.DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn
)

# =============== æ¨¡å‹ç»„ä»¶å‡†å¤‡ ===============
vae = model.first_stage_model         # å˜åˆ†è‡ªç¼–ç å™¨
text_encoder = model.cond_stage_model # æ–‡æœ¬ç¼–ç å™¨
unet = model.model.diffusion_model    # æ‰©æ•£æ¨¡å‹ï¼ˆUNetï¼‰

# å†»ç»“ä¸éœ€è¦è®­ç»ƒçš„ç»„ä»¶
vae.requires_grad_(False)
if not train_text_encoder:
    text_encoder.requires_grad_(False)

# =============== ä¼˜åŒ–å™¨è®¾ç½® ===============
optimizer_class = torch.optim.AdamW
params_to_optimize = (
    itertools.chain(unet.parameters(),
                    text_encoder.parameters()) if train_text_encoder else unet.parameters())
optimizer = optimizer_class(
    params_to_optimize,
    lr=learning_rate,
    betas=(adam_beta1, adam_beta2),
    weight_decay=adam_weight_decay,
    eps=adam_epsilon,
)

# =============== æ··åˆç²¾åº¦è®­ç»ƒ ===============
scaler = GradScaler()  # æ¢¯åº¦ç¼©æ”¾å™¨ï¼ˆç”¨äºæ··åˆç²¾åº¦è®­ç»ƒï¼‰

# å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
model.to(device)
vae = model.first_stage_model.to(device)
text_encoder = model.cond_stage_model.to(device)
unet = model.model.diffusion_model.to(device)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_train_epochs):
    unet.train()
    if train_text_encoder:
        text_encoder.train()
    for step, batch in enumerate(train_dataloader):
        # è·å–è¾“å…¥æ•°æ®
        x, c = model.get_input(batch, "pixel_values")

        # æ··åˆç²¾åº¦è®­ç»ƒ
        with autocast():  # è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            if with_prior_preservation:
                # åˆ†ç¦»å®ä¾‹å’Œå…ˆéªŒæ•°æ®
                x, x_prior = torch.chunk(x, 2, dim=0)
                c, c_prior = torch.chunk(c, 2, dim=0)
                # è®¡ç®—å®ä¾‹æŸå¤±å’Œå…ˆéªŒæŸå¤±
                loss, _ = model(x, c)
                prior_loss, _ = model(x_prior, c_prior)
                # ç»„åˆæŸå¤±
                loss = loss + prior_loss_weight * prior_loss
            else:
                # ä»…è®¡ç®—å®ä¾‹æŸå¤±
                loss, _ = model(x, c)

        print('*' * 20, "loss=", str(loss.detach().item()))

        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        scaler.scale(loss).backward()  # ç¼©æ”¾æ¢¯åº¦
        scaler.step(optimizer)        # æ›´æ–°å‚æ•°
        scaler.update()               # æ›´æ–°ç¼©æ”¾å™¨
        optimizer.zero_grad()         # æ¸…é›¶æ¢¯åº¦

# =============== ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ ===============
checkpoint_path = './checkpoints/AltDiffusion-m18-new-trained/model.ckpt'
# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
torch.save(model.state_dict(), checkpoint_path)
print(f"Model saved to {checkpoint_path}")
